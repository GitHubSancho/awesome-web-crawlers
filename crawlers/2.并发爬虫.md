### 异步下载器
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: functions.py
#CREATE_TIME: 2022-07-23
#AUTHOR: Sancho
"""
实现异步downloader
"""


import re
import urllib.parse as urlparse
import requests
import cchardet
import traceback


async def fetch(session, url, headers=None, timeout=9, binary=False):
    """解析网页，返回状态码、网页、回链"""
    _headers = {
        'User-Agent': ('Mozilla/5.0 (compatible; MSIE 9.0; '
                       'Windows NT 6.1; Win64; x64; Trident/5.0)'),
    }
    if headers:
        _headers = headers
    try:
        async with session.get(url, headers=_headers, timeout=timeout) as response: # 获取网页对话
            status = response.status
            html = await response.read() # 异步读取网页
            if not binary: # 编码识别
                encoding = cchardet.detect(html)['encoding']
                html = html.decode(encoding, errors='ignore')
            redirected_url = str(response.url) # 回链
    except Exception as e: # 处理错误信息
        msg = 'Failed download: {} | exception: {}, {}'.format(url, str(type(e)), str(e))
        print(msg)
        html = ''
        status = 0
        redirected_url = url
    return status, html, redirected_url


def downloader(url, timeout=10, headers=None, debug=False, binary=False):
    """非异步下载器"""
    _headers = {
        'User-Agent': ('Mozilla/5.0 (compatible; MSIE 9.0; '
                       'Windows NT 6.1; Win64; x64; Trident/5.0)'),
    }
    redirected_url = url
    if headers:
        _headers = headers
    try:
        r = requests.get(url, headers=_headers, timeout=timeout)
        if binary:
            html = r.content
        else:
            encoding = cchardet.detect(r.content)['encoding']
            html = r.content.decode(encoding, errors='ignore')
        status = r.status_code
        redirected_url = r.url
    except:
        if debug:
            traceback.print_exc()
        msg = 'failed download: {}'.format(url)
        print(msg)
        if binary:
            html = b''
        else:
            html = ''
        status = 0
    return status, html, redirected_url


g_bin_postfix = set([
    'exe', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx',
    'pdf',
    'jpg', 'png', 'bmp', 'jpeg', 'gif',
    'zip', 'rar', 'tar', 'bz2', '7z', 'gz',
    'flv', 'mp4', 'avi', 'wmv', 'mkv',
    'apk',
]) # 数据格式

g_news_postfix = [
    '.html?', '.htm?', '.shtml?',
    '.shtm?',
] # 网页格式


def clean_url(url):
    # 1. 是否为合法的http url
    if not url.startswith('http'):
        return ''
    # 2. 去掉静态化url后面的参数
    for np in g_news_postfix:
        p = url.find(np)
        if p > -1:
            p = url.find('?')
            url = url[:p]
            return url
    # 3. 不下载二进制类内容的链接
    up = urlparse.urlparse(url)
    path = up.path
    if not path:
        path = '/'
    postfix = path.split('.')[-1].lower()
    if postfix in g_bin_postfix:
        return ''

    # 4. 去掉标识流量来源的参数
    # badquery = ['spm', 'utm_source', 'utm_source', 'utm_medium', 'utm_campaign']
    good_queries = []
    for query in up.query.split('&'):
        qv = query.split('=')
        if qv[0].startswith('spm') or qv[0].startswith('utm_'):
            continue
        if len(qv) == 1:
            continue
        good_queries.append(query)
    query = '&'.join(good_queries)
    url = urlparse.urlunparse((
        up.scheme,
        up.netloc,
        path,
        up.params,
        query,
        ''  #  crawler do not care fragment
    ))
    return url


g_pattern_tag_a = re.compile(r'<a[^>]*?href=[\'"]?([^> \'"]+)[^>]*?>(.*?)</a>', re.I|re.S|re.M)


def extract_links_re(url, html):
    '''正则提取网页内链接'''
    newlinks = set()
    aa = g_pattern_tag_a.findall(html)
    for a in aa:
        link = a[0].strip()
        if not link:
            continue
        link = urlparse.urljoin(url, link)
        link = clean_url(link)
        if not link:
            continue
        newlinks.add(link)
    return newlinks


def init_file_logger(fname):
    """生成日志文件"""
    import logging
    from logging.handlers import TimedRotatingFileHandler
    ch = TimedRotatingFileHandler(fname, when="midnight")
    ch.setLevel(logging.INFO)
    # 创建格式化
    fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    formatter = logging.Formatter(fmt)
    # 将格式化应用到ch对象
    ch.setFormatter(formatter)
    logger = logging.getLogger(fname)
    # 将ch对象添加到记录器
    logger.addHandler(ch)
    return logger



if __name__ == '__main__':
    url = 'http://news.baidu.com/'
    s, html = downloader(url)
    print(s, len(html))

```

```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: news-crawler-async.py
#CREATE_TIME: 2022-07-23
#AUTHOR: Sancho
"""
实现异步并发爬虫
!:需要linux
!TODO:更换数据库
"""

import traceback
import time
import asyncio
import aiohttp
import urllib.parse as urlparse
import farmhash
import lzma

import uvloop  #需要linux
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

import sanicdb  # aiomysql的封装

from urlpool import UrlPool
import functions as fn
import config


class NewsCrawlerAsync:
    def __init__(self, name):
        self._workers = 0
        self._workers_max = 30 # 最大协程数
        self.logger = fn.init_file_logger(name + '.log') # 获取日志对象

        self.urlpool = UrlPool(name) # 获取网址池

        # 通过同一个事件循环管理协程
        self.loop = asyncio.get_event_loop() # 获取事件循环
        self.session = aiohttp.ClientSession(loop=self.loop)
        self.db = sanicdb.SanicDB(config.db_host,
                                  config.db_db,
                                  config.db_user,
                                  config.db_password,
                                  loop=self.loop) # 连接数据库

    async def load_hubs(self, ):
        """读取hubs的url"""
        sql = 'select url from crawler_hub' # sql命令从hub中选取url
        data = await self.db.query(sql)
        self.hub_hosts = set()
        hubs = []
        for d in data:
            host = urlparse.urlparse(d['url']).netloc # 解析域名
            self.hub_hosts.add(host)
            hubs.append(d['url'])
        self.urlpool.set_hubs(hubs, 300) # 设置首页中的链接到网址池

    async def save_to_db(self, url, html):
        """异步存储网址和网页"""
        urlhash = farmhash.hash64(url)
        sql = 'select url from crawler_html where urlhash=%s'
        d = await self.db.get(sql, urlhash)
        if d:
            if d['url'] != url:
                msg = 'farmhash collision: %s <=> %s' % (url, d['url'])
                self.logger.error(msg)
            return True
        if isinstance(html, str):
            html = html.encode('utf8')
        html_lzma = lzma.compress(html)
        sql = ('insert into crawler_html(urlhash, url, html_lzma) '
               'values(%s, %s, %s)')
        good = False
        try:
            await self.db.execute(sql, urlhash, url, html_lzma)
            good = True
        except Exception as e:
            if e.args[0] == 1062:
                # Duplicate entry
                good = True
                pass
            else:
                traceback.print_exc()
                raise e
        return good

    def filter_good(self, urls):
        """简单判断网址是否正确"""
        goodlinks = []
        for url in urls:
            host = urlparse.urlparse(url).netloc # 解析域名
            if host in self.hub_hosts:
                goodlinks.append(url)
        return goodlinks

    async def process(self, url, ishub):
        """解析url"""
        status, html, redirected_url = await fn.fetch(self.session, url) # 异步解析网页获取对话
        self.urlpool.set_status(url, status)
        if redirected_url != url:
            self.urlpool.set_status(redirected_url, status)
        # 提取hub网页中的链接, 新闻网页中也有“相关新闻”的链接，按需提取
        if status != 200:
            return
        if ishub:
            newlinks = fn.extract_links_re(redirected_url, html) # 正则提取网页内链接
            goodlinks = self.filter_good(newlinks) # 判断网址是否正确
            print("%s/%s, goodlinks/newlinks" %
                  (len(goodlinks), len(newlinks)))
            self.urlpool.addmany(goodlinks) # 将多个链接添加到网址池
        else:
            await self.save_to_db(redirected_url, html) # 保存到数据库
        self._workers -= 1 # 释放协程计数

    def load_config(self):
        """
        !TODO: 读取配置 
        """
        pass

    async def loop_crawl(self, ):
        """异步的主调度器"""
        await self.load_hubs()
        last_rating_time = time.time()
        counter = 0 # 计数器
        while 1:
            tasks = self.urlpool.pop(self._workers_max) # 取出待下载url
            if not tasks:
                print('no url to crawl, sleep')
                await asyncio.sleep(3)
                continue
            for url, ishub in tasks.items():
                self._workers += 1
                counter += 1 # 计数器 +1
                print('crawl:', url)
                asyncio.ensure_future(self.process(url, ishub)) # 异步解析url

            gap = time.time() - last_rating_time
            if gap > 5: 
                rate = counter / gap
                print('\tloop_crawl() rate:%s, counter: %s, workers: %s' %
                      (round(rate, 2), counter, self._workers))
                last_rating_time = time.time()
                counter = 0
            if self._workers > self._workers_max: # 超过最大协程量
                print(
                    '====== got workers_max, sleep 3 sec to next worker =====')
                await asyncio.sleep(3)

    def run(self):
        try:
            self.loop.run_until_complete(self.loop_crawl())
        except KeyboardInterrupt:
            print('stopped by yourself!')
            del self.urlpool # 退出并写入缓存
            pass


if __name__ == '__main__':
    nc = NewsCrawlerAsync('crawlers_sancho')
    nc.run()
```
