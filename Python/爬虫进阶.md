# 爬虫进阶知识

### url解析
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: test_urlparse.py
#CREATE_TIME: 2022-07-14
#AUTHOR: Sancho

import urllib.parse as urlparse

url = "https://www.163.com/news/article/HC853UKG000189FH.html"
z = urlparse.urlparse(url)
print(z)

# 输出：
# ParseResult(scheme='https', netloc='www.163.com', path='/news/article/HC853UKG000189FH.html', params='', query='', fragment='')
```
### 编码识别
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: test_textparse.py
#CREATE_TIME: 2022-07-14
#AUTHOR: Sancho

import requests
import chardet
import cchardet

url = "https://www.baidu.com"
resp = requests.get(url)

# print(resp.text)  # 直接打印文本时会出现乱码
# print(resp.encoding) # 查看编码

code = chardet.detect(resp.content)['encoding']  # 识别二进制的文本编码方式
print(resp.content.decode(code))  # 指定编码

print(resp.content.decode(cchardet.detect(
    resp.content)["encoding"]))  # 更健全的编码识别模块
```
### 简单爬虫设计
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: test_downloader.py
#CREATE_TIME: 2022-07-14
#AUTHOR: Sancho
"""
实现一个简单的下载器
"""

import requests
import cchardet
import traceback


def downloader(url, timeout=10, headers=None, debug=False, binary=False):
    _headers = {
        'User-Agent':
        ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'
         ),
    }
    if headers:  # 判断ua是否传递，没有则选择上面ua
        _headers = headers

    redirected_url = url

    try:
        # 尝试访问网页
        resp = requests.get(url, headers=_headers, timeout=timeout)
        if binary:  # 是否需要二进制文件
            html = resp.content
        else:
            html = resp.content.decode(
                cchardet.detect(resp.content)['encoding'])  # 识别并改变编码

        status = resp.status_code
        redirected_url = resp.url  # 重定向url

    except:
        # 错误检测
        if debug:
            traceback.print_exc()  # 打印错误行
        msg = 'failed download: {}'.format(url)
        print(msg)

        if binary:
            html = b''
        else:
            html = ''
        status = 0
    return status, html, redirected_url


if __name__ == '__main__':
    url = 'http://news.baidu.com/'
    s_code, html, r_url = downloader(url)
    print(s_code, len(html), r_url)
```
### 数据库交互
```python
TODO:写入mysql，redis，mongodb的基本数据库操作代码
```
### 网址数据存储
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: test_urldb.py
#CREATE_TIME: 2022-07-14
#AUTHOR: Sancho
"""
网址数据库实现
!需要linux
TODO:更换成其它数据库
"""

import leveldb


class UrlDB:
    """
    使用数据库储存已完成的url
    """
    status_failure = b'0'
    status_success = b'1'

    def __init__(self, db_name):
        self.name = db_name + '.urldb'
        self.db = leveldb.LevelDB(self.name) # 连接数据库

    def set_success(self, url):
        """添加成功的数据"""
        if isinstance(url, str): # 判断数据是否是字符串
            url = url.encode('utf8')
        try:
            self.db.Put(url, self.status_success) # 尝试写入数据
            s = True
        except:
            s = False
        return s

    def set_failure(self, url):
        """添加失败的数据"""
        if isinstance(url, str):
            url = url.encode('utf8')
        try:
            self.db.Put(url, self.status_failure)
            s = True
        except:
            s = False
        return s

    def has(self, url):
        """判断数据是否已存在数据库"""
        if isinstance(url, str):
            url = url.encode('utf8')
        try:
            attr = self.db.Get(url)
            return attr
        except:
            pass
        return False
```
### 网址池
```python
#!/usr/bin/env python
#-*- coding: utf-8 -*-
#FILE: test_urlpool.py
#CREATE_TIME: 2022-07-14
#AUTHOR: Sancho
"""
网址池实现
!需要linux
TODO:更换成其它数据库
TODO:替换urllib.parse代码
"""

import pickle
import leveldb
import time
import urllib.parse as urlparse


class UrlPool:
    '''
    用于管理url的网址池
    '''

    def __init__(self, pool_name):
        self.name = pool_name
        self.db = UrlDB(pool_name) # 载入网址数据库

        self.waiting = {}  # {host: set([urls]), } 按host分组，记录等待下载的URL
        self.pending = {}  # {url: pended_time, } 记录已被取出（self.pop()）但还未被更新状态（正在下载）的URL
        self.failure = {}  # {url: times,} 记录失败的URL的次数
        self.failure_threshold = 3 # 允许的最大失败次数
        self.pending_threshold = 10  # pending的最大时间，过期要重新下载
        self.waiting_count = 0  # self.waiting 字典里面的url的个数
        self.max_hosts = ['', 0]  # [host: url_count] 目前pool中url最多的host及其url数量
        self.hub_pool = {}  # {url: last_query_time, }  存放hub url（首页的链接）
        self.hub_refresh_span = 0 # 爬取的时间频率
        self.load_cache() # 读取上次未完成抓取网址的数据

    def __del__(self):
        """退出时自动调用写入缓存"""
        self.dump_cache()

    def load_cache(self,):
        """读取上次未完成抓取网址的数据"""
        path = self.name + '.pkl'
        try:
            with open(path, 'rb') as f:
                self.waiting = pickle.load(f) # 读到内存
            cc = [len(v) for k, v in self.waiting.items()] # 数据字段计数
            print('加载到网址池的网址:', sum(cc))
        except:
            pass

    def dump_cache(self):
        """写入缓存"""
        path = self.name + '.pkl'
        try:
            with open(path, 'wb') as f:
                pickle.dump(self.waiting, f) # 将未完成抓取的网址，序列化写入硬盘
            print('self.waiting saved!')
        except:
            pass

    def set_hubs(self, urls, hub_refresh_span):
        """加载首页中的链接到网址池"""
        self.hub_refresh_span = hub_refresh_span
        self.hub_pool = {}
        for url in urls:
            self.hub_pool[url] = 0

    def set_status(self, url, status_code):
        """访问链接并设置状态"""
        if url in self.pending:
            self.pending.pop(url) # 取出已下载的、待更新的url

        if status_code == 200:
            self.db.set_success(url) # 写入到成功的网址池
            return
        if status_code == 404:
            self.db.set_failure(url) # （地址不存在）写入到失败的网址池
            return
        if url in self.failure: # 其它状态时判断是否已经在失败的网址池中
            self.failure[url] += 1 # 记录失败次数+1
            if self.failure[url] > self.failure_threshold: # 判断失败次数是否超过上限
                self.db.set_failure(url) # 从数据库中设置失败状态
                self.failure.pop(url) # 从失败的网址池中销毁
            else: # 没有达到失败上限
                self.add(url) # 重新加载（放入self.waittig）
        else: # 第一次失败
            self.failure[url] = 1
            self.add(url) # 重新加载（放入self.waittig）

    def push_to_pool(self, url):
        """将url按host分组放入self.waiting"""
        host = urlparse.urlparse(url).netloc # 解析到主机地址
        if not host or '.' not in host: # 判断主机地址是否合法
            print('地址错误:', url, ', len of ur:', len(url))
            return False
        if host in self.waiting: # 判断主机地址是否已被记录
            if url in self.waiting[host]: # 判断链接是否已经在待下载池中
                return True
            self.waiting[host].add(url) # 添加链接到待下载池中
            if len(self.waiting[host]) > self.max_hosts[1]: # 判断host下的链接是否是最多的
                self.max_hosts[1] = len(self.waiting[host]) # 刷新host的最多数量
                self.max_hosts[0] = host
        else: # 如果不存在待下载队列（新链接）
            self.waiting[host] = set([url]) # 加入待下载队列
        self.waiting_count += 1 # 计数器+1
        return True

    def add(self, url, always=False):
        """将链接添加到网址池"""
        if always: # 强制放入待下载队列
            return self.push_to_pool(url)
        pended_time = self.pending.get(url, 0) # 查看上次下载的时间
        if time.time() - pended_time < self.pending_threshold: # 是否在刷新间隔
            print('正在下载:', url)
            return
        if self.db.has(url): # 链接是否在数据库
            return
        if pended_time: # 超过刷新间隔
            self.pending.pop(url) # 弹出
        return self.push_to_pool(url) # 放入self.waiting

    def addmany(self, urls, always=False):
        """将多个链接添加到网址池"""
        if isinstance(urls, str): # 判断是否是字符串类型
            print('urls是字符串，请传入多个链接的可迭代对象！', urls)
            self.add(urls, always)
        else:
            for url in urls:
                self.add(url, always) # 遍历将url添加到网址池

    def pop(self, count, hub_percent=50):
        """
        弹出链接进入下载
        count:需要弹出的链接个数（并发）
        hub_percent:弹出的链接中hub_url的占比
        """
        print('\n\tmax of host:', self.max_hosts)

        # 取出的url有两种类型：hub=1, 普通=0
        url_attr_url = 0 # 0表示普通url
        url_attr_hub = 1 # 1表示hub_url

        # 1. 首先取出hub，保证获取hub里面的最新url.
        hubs = {}
        hub_count = count * hub_percent // 100 # 计算需要弹出的hub_url个数
        for hub in self.hub_pool:
            span = time.time() - self.hub_pool[hub] # 计算到上次弹出时的时间差
            if span < self.hub_refresh_span: # 判断时间差是否在刷新间隔
                continue
            hubs[hub] = url_attr_hub 
            self.hub_pool[hub] = time.time() # 更新时间戳
            if len(hubs) >= hub_count: # 计算弹出个数
                break

        # 2. 再取出普通url
        left_count = count - len(hubs) # 计算需要弹出的普通url个数
        urls = {}
        for host in self.waiting: # 遍历待下载的队列
            if not self.waiting[host]: # 判断当前host下是否没有链接待爬取
                continue
            url = self.waiting[host].pop() # 弹出url
            urls[url] = url_attr_url # 标记为普通url
            self.pending[url] = time.time() # 更新时间戳
            if self.max_hosts[0] == host: # 判断是否是最多链接数的host
                self.max_hosts[1] -= 1
            if len(urls) >= left_count: # 达到需要弹出的数量
                break
        self.waiting_count -= len(urls) # 更新计数器
        print('To pop:%s, hubs: %s, urls: %s, hosts:%s' % (count, len(hubs), len(urls), len(self.waiting)))
        urls.update(hubs) # 合并urls和hubs两个字典
        return urls

    def size(self,):
        """返回链接数"""
        return self.waiting_count

    def empty(self,):
        """查看待爬取的地址池是否为空"""
        return self.waiting_count == 0
```
